{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code Section 1:  Model Training and Testing\n",
    "## This section contains two challenges"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For time series data, we should convert the raw dataset to feature dataset, where each data sample contains features extracted from a period of time. For example, for each 1000 sensor data records/rows,we can consider it as a segmentation (Window) and extract statistic features from it. In this tutorial, we extract min， max and mean values of the first acceleromter on the wrist sensor. In data visualization used in the previous weeks, we could find that for different activities the sensor signal data values are in different ranges. Therefore, we could think that we could recognize different activities by the range of data, which means minimum,maximum and mean values of data may be useful features to recognize activities.     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File dataset_1.txt has 12 windows, each of which has 1000 rows for activity: 1\n",
      "File dataset_1.txt has 12 windows, each of which has 1000 rows for activity: 2\n",
      "File dataset_1.txt has 12 windows, each of which has 1000 rows for activity: 3\n",
      "File dataset_1.txt has 24 windows, each of which has 1000 rows for activity: 4\n",
      "File dataset_1.txt has 12 windows, each of which has 1000 rows for activity: 5\n",
      "File dataset_1.txt has 18 windows, each of which has 1000 rows for activity: 6\n",
      "File dataset_1.txt has 52 windows, each of which has 1000 rows for activity: 7\n",
      "File dataset_1.txt has 6 windows, each of which has 1000 rows for activity: 8\n",
      "File dataset_1.txt has 6 windows, each of which has 1000 rows for activity: 9\n",
      "File dataset_1.txt has 25 windows, each of which has 1000 rows for activity: 10\n",
      "File dataset_1.txt has 24 windows, each of which has 1000 rows for activity: 11\n",
      "File dataset_1.txt has 24 windows, each of which has 1000 rows for activity: 12\n",
      "File dataset_1.txt has 12 windows, each of which has 1000 rows for activity: 13\n",
      "File dataset_1.txt has 12 windows, each of which has 1000 rows for activity: 1\n",
      "File dataset_1.txt has 12 windows, each of which has 1000 rows for activity: 2\n",
      "File dataset_1.txt has 12 windows, each of which has 1000 rows for activity: 3\n",
      "File dataset_1.txt has 24 windows, each of which has 1000 rows for activity: 4\n",
      "File dataset_1.txt has 12 windows, each of which has 1000 rows for activity: 5\n",
      "File dataset_1.txt has 18 windows, each of which has 1000 rows for activity: 6\n",
      "File dataset_1.txt has 52 windows, each of which has 1000 rows for activity: 7\n",
      "File dataset_1.txt has 6 windows, each of which has 1000 rows for activity: 8\n",
      "File dataset_1.txt has 6 windows, each of which has 1000 rows for activity: 9\n",
      "File dataset_1.txt has 25 windows, each of which has 1000 rows for activity: 10\n",
      "File dataset_1.txt has 24 windows, each of which has 1000 rows for activity: 11\n",
      "File dataset_1.txt has 24 windows, each of which has 1000 rows for activity: 12\n",
      "File dataset_1.txt has 12 windows, each of which has 1000 rows for activity: 13\n",
      "attempted to create training feature set data file:week6_training_data_1Participant.csv. Please check if the file was created successfully in your local folder!\n",
      "478 data rows should be in the output training feature set file\n",
      "File dataset_3.txt has 12 windows, each of which has 1000 rows for activity: 1\n",
      "File dataset_3.txt has 12 windows, each of which has 1000 rows for activity: 2\n",
      "File dataset_3.txt has 12 windows, each of which has 1000 rows for activity: 3\n",
      "File dataset_3.txt has 24 windows, each of which has 1000 rows for activity: 4\n",
      "File dataset_3.txt has 12 windows, each of which has 1000 rows for activity: 5\n",
      "File dataset_3.txt has 15 windows, each of which has 1000 rows for activity: 6\n",
      "File dataset_3.txt has 55 windows, each of which has 1000 rows for activity: 7\n",
      "File dataset_3.txt has 9 windows, each of which has 1000 rows for activity: 8\n",
      "File dataset_3.txt has 7 windows, each of which has 1000 rows for activity: 9\n",
      "File dataset_3.txt has 24 windows, each of which has 1000 rows for activity: 10\n",
      "File dataset_3.txt has 24 windows, each of which has 1000 rows for activity: 11\n",
      "File dataset_3.txt has 24 windows, each of which has 1000 rows for activity: 12\n",
      "File dataset_3.txt has 9 windows, each of which has 1000 rows for activity: 13\n",
      "attempted to create testing feature set data file:week6_testing_data_1Participant.csv. Please check if the file was created successfully in your local folder!\n",
      "239 data rows should be in the output testing set file\n",
      "Accuracy:  0.4225941422594142\n",
      "[[ 0  1  0  1  0  0  0  0  0  0 10  0  0]\n",
      " [ 0 12  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  3  1  0  0  0  8  0  0  0  0  0  0]\n",
      " [ 0  0  0 16  1  7  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0 10  1  0  0  1  0  0  0  0]\n",
      " [ 0  0  0  4  4  7  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  3  2  0  0  0 50  0  0  0  0]\n",
      " [ 0  0  0  0  1  0  0  0  8  0  0  0  0]\n",
      " [ 0  0  0  0  4  0  0  0  3  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0 24  0  0  0]\n",
      " [ 0  0  0  4  0  0  0  0  0  0 18  2  0]\n",
      " [ 0  1  0  2  0  0  0  0  0  0 20  1  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  9]]\n"
     ]
    }
   ],
   "source": [
    "#Code Block 1.1\n",
    "\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "from scipy import signal\n",
    "import matplotlib.pyplot as plt \n",
    "import math\n",
    "from sklearn import preprocessing\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import make_scorer, accuracy_score, confusion_matrix\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "def create_training_data_from_files(list_of_filenames, output_filename):\n",
    "\n",
    "    #create the empty training set where we are going to add our \"features\"\n",
    "    training_set = np.empty(shape=(0, 10))\n",
    "    \n",
    "    for dataset_file in list_of_filenames:\n",
    "\n",
    "        #import the file contents into a panadas data frame\n",
    "        imported_data = pd.read_csv(dataset_file, sep=',', header=None)\n",
    "\n",
    "        #generate \"features\" for each activitiy\n",
    "        for activityNumber in range(1,14):\n",
    "            \n",
    "            #get all data relating to that activity and convert to a numpy ndarray\n",
    "            activity_data = imported_data[imported_data[24] == activityNumber].values\n",
    "\n",
    "            #smooth over the data for columns 0, 1, 2, ...23 (not column 24)\n",
    "            b, a = signal.butter(4, 0.04, 'low', analog=False)\n",
    "            for j in range(24):\n",
    "                activity_data[:, j] = signal.lfilter(b, a, activity_data[:, j])\n",
    "            \n",
    "            #how many full rows of 1000 are there for this activity data?\n",
    "            number_of_training_windows = int( len(activity_data)/1000 )\n",
    "            print(  \"File \" + dataset_file +\n",
    "                    \" has \" + str(number_of_training_windows) + \" windows, each of which has 1000 rows \"+\n",
    "                    \"for activity: \" + str(activityNumber))\n",
    "            \n",
    "            #for each window of 1000 rows... scan the data and add the scan results to training_set\n",
    "            for window_number in range(number_of_training_windows):\n",
    "                #sample data (get the next 1000 rows and all the columns)\n",
    "                window_data = activity_data[ \n",
    "                                1000 * window_number : 1000 * (window_number + 1) , \n",
    "                                :\n",
    "                            ]\n",
    "                #we are about to build up a feature_sample that will have 10 columns\n",
    "                feature_window = []\n",
    "                for i in range(3):\n",
    "                    feature_window.append(np.min(window_data[:, i]))\n",
    "                    feature_window.append(np.max(window_data[:, i]))\n",
    "                    feature_window.append(np.mean(window_data[:, i]))\n",
    "                # add the activtiy number (The last column from the row of data)\n",
    "                feature_window.append(int(window_data[0, -1])) \n",
    "                #make it in to an ndarray so it can be added to training data\n",
    "                feature_window = np.array([feature_window]) \n",
    "                training_set = np.concatenate((training_set, feature_window), axis=0)\n",
    "            \n",
    "    #now save all this training data into a file to be used at a later date\n",
    "    df_training = pd.DataFrame(training_set)\n",
    "    df_training.to_csv(output_filename, index=None, header=None)\n",
    "    print('attempted to create training feature set data file:'+ output_filename +'. Please check if the file was created successfully in your local folder!')\n",
    "    print(str(len(training_set)) + \" data rows should be in the output training feature set file\")\n",
    "\n",
    "\n",
    "def create_testing_data_from_files(list_of_filenames, output_filename):\n",
    "\n",
    "    #create the empty training set where we are going to add our \"features\"\n",
    "    testing_set = np.empty(shape=(0, 10))\n",
    "    \n",
    "    for dataset_file in list_of_filenames:\n",
    "\n",
    "        #import the file contents into a panadas data frame\n",
    "        imported_data = pd.read_csv(dataset_file, sep=',', header=None)\n",
    "\n",
    "        #generate \"features\" for each activitiy\n",
    "        for activityNumber in range(1,14):\n",
    "            \n",
    "            #get all data relating to that activity and convert to a numpy ndarray\n",
    "            activity_data = imported_data[imported_data[24] == activityNumber].values\n",
    "\n",
    "            #smooth over the data for columns 0, 1, 2, ...23 (not column 24)\n",
    "            b, a = signal.butter(4, 0.04, 'low', analog=False)\n",
    "            for j in range(24):\n",
    "                activity_data[:, j] = signal.lfilter(b, a, activity_data[:, j])\n",
    "            \n",
    "            #how many full rows of 1000 are there for this activity data?\n",
    "            number_of_testing_windows = int( len(activity_data)/1000 )\n",
    "            print(  \"File \" + dataset_file +\n",
    "                    \" has \" + str(number_of_testing_windows) + \" windows, each of which has 1000 rows \"+\n",
    "                    \"for activity: \" + str(activityNumber))\n",
    "            \n",
    "            #for each window of 1000 rows... scan the data and add the scan results to training_set\n",
    "            for window_number in range(number_of_testing_windows):\n",
    "                #sample data (get the next 1000 rows and all the columns)\n",
    "                window_data = activity_data[ \n",
    "                                1000 * window_number : 1000 * (window_number + 1) , \n",
    "                                :\n",
    "                            ]\n",
    "                #we are about to build up a feature_sample that will have 10 columns\n",
    "                feature_window = []\n",
    "                for i in range(3):\n",
    "                    feature_window.append(np.min(window_data[:, i]))\n",
    "                    feature_window.append(np.max(window_data[:, i]))\n",
    "                    feature_window.append(np.mean(window_data[:, i]))\n",
    "                # add the activtiy number (The last column from the row of data)\n",
    "                feature_window.append(int(window_data[0, -1])) \n",
    "                #make it in to an ndarray so it can be added to training data\n",
    "                feature_window = np.array([feature_window]) \n",
    "                testing_set = np.concatenate((testing_set, feature_window), axis=0)\n",
    "            \n",
    "    #now save all this training data into a file to be used at a later date\n",
    "    df_testing = pd.DataFrame(testing_set)\n",
    "    df_testing.to_csv(output_filename, index=None, header=None)\n",
    "    print('attempted to create testing feature set data file:'+ output_filename +'. Please check if the file was created successfully in your local folder!')\n",
    "    print(str(len(testing_set)) + \" data rows should be in the output testing set file\")\n",
    "    \n",
    "    training_fileName = []\n",
    "training_fileName.append ('dataset_1.txt')\n",
    "testing_fileName = []\n",
    "testing_fileName.append ('dataset_3.txt')\n",
    "create_training_data_from_files (training_fileName, 'week6_training_data_1Participant.csv')\n",
    "create_testing_data_from_files (testing_fileName, 'week6_testing_data_1Participant.csv')\n",
    "\n",
    "df_training = pd.read_csv('week6_training_data_1Participant.csv', header=None)\n",
    "df_testing = pd.read_csv('week6_testing_data_1Participant.csv', header=None)\n",
    "\n",
    "label_train = df_training[9].values\n",
    "# Labels should start from 0 in sklearn\n",
    "label_train = label_train - 1\n",
    "df_training = df_training.drop([9], axis=1)\n",
    "data_train = df_training.values\n",
    "\n",
    "label_test = df_testing[9].values\n",
    "label_test = label_test - 1\n",
    "df_testing = df_testing.drop([9], axis=1)\n",
    "data_test = df_testing.values\n",
    "\n",
    "# Feature normalization for improving the performance of machine learning models. In this example code, \n",
    "# StandardScaler is used to scale original feature to be centered around zero. You could try other normalization methods.\n",
    "scaler = preprocessing.StandardScaler().fit(data_train)\n",
    "data_train = scaler.transform(data_train)\n",
    "data_test = scaler.transform(data_test)\n",
    "\n",
    "# Build KNN classifier, in this example code\n",
    "knn = KNeighborsClassifier(n_neighbors=3)\n",
    "knn.fit(data_train, label_train)\n",
    "\n",
    "# Evaluation. when we train a machine learning model on training set, we should evaluate its performance on testing set.\n",
    "# We could evaluate the model by different metrics. Firstly, we could calculate the classification accuracy.\n",
    "label_pred = knn.predict(data_test)\n",
    "print('Accuracy: ', accuracy_score(label_test, label_pred))\n",
    "# We could use confusion matrix to view the classification for each activity.\n",
    "print(confusion_matrix(label_test, label_pred))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Challenge 1\n",
    "\n",
    "\n",
    "#Question 1\n",
    "#\n",
    "#After executing code block 1.1, do you notice there are two new files generated in your local folder?\n",
    "#what are the difference between these two files? (Hint: think from the lecture, what is the difference between \n",
    "#model training and model inference?)\n",
    "#\n",
    "#A trainig_data1 file and a testing_data1 file was created.\n",
    "#A training data is the initial dataset you use to teach a model or recognise patterns or perform to a criteria.\n",
    "#A testing data is a dataset to validate the model.\n",
    "\n",
    "#Question 2\n",
    "#\n",
    "#After executing code block 1.1, you see an accuracy value (Accuracy：x) in the output? How was it calculated? \n",
    "#\n",
    "#The accuracy was 0.4225941422594142.\n",
    "\n",
    "\n",
    "\n",
    "#Question 3\n",
    "#\n",
    "#After executing code block 1.1, there is an matrix output. Why it takes a while to output? What are the columns \n",
    "#and what are the rows?  \n",
    "#\n",
    "#It takes a while as there are large amount of data being compared.\n",
    "#The columns are predictions for a specific activity, so the first column is the prediction for activity 1, second colmn is predictio for activity 2, etc.\n",
    "#The rows are the actual number of the activity, so the first row is the number of activity 1, second row is number of activity 2, etc.\n",
    "#Value in columns that aren't on the diagonal are false positives. \n",
    "#Value in rows that aren't on the diagonal are true negatives.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Challenge 2\n",
    "\n",
    "\n",
    "#Question 1\n",
    "#\n",
    "#What is the accuracy for the sitting activity (please refer to the dataset link or the final pages of \n",
    "#the week 6 lecture)?\n",
    "\n",
    "\n",
    "\n",
    "#Question 2\n",
    "#\n",
    "#What are those activities where the accuracies are not so well (e.g., false positive + false negative >= 5)?\n",
    "\n",
    "\n",
    "\n",
    "#Question 3\n",
    "#\n",
    "#Any reasons why the above-mentioned activities have bad accuracies? and what are your suggestions to improve \n",
    "#the accuracies?\n",
    "#\n",
    "#A potential reason for the bad accuracy might be because there aren't enough training data (for activity 9).\n",
    "#And some activities are hard to distinguish with one another (like acitivity 11 and 12).\n",
    "#Increasing the number of training_dataset will 'train' the model more hence making it more accurate.\n",
    "#And adjusting the k-value in k-NN algorithm will improve the accuracy.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code Section 2: Testing with model parameter (T) and training data (E)\n",
    "## This section contains one challenge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### in the code block below, we changed the K neighbours in KNN to 4 instead of 3 in the pervious code section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File dataset_1.txt has 12 windows, each of which has 1000 rows for activity: 1\n",
      "File dataset_1.txt has 12 windows, each of which has 1000 rows for activity: 2\n",
      "File dataset_1.txt has 12 windows, each of which has 1000 rows for activity: 3\n",
      "File dataset_1.txt has 24 windows, each of which has 1000 rows for activity: 4\n",
      "File dataset_1.txt has 12 windows, each of which has 1000 rows for activity: 5\n",
      "File dataset_1.txt has 18 windows, each of which has 1000 rows for activity: 6\n",
      "File dataset_1.txt has 52 windows, each of which has 1000 rows for activity: 7\n",
      "File dataset_1.txt has 6 windows, each of which has 1000 rows for activity: 8\n",
      "File dataset_1.txt has 6 windows, each of which has 1000 rows for activity: 9\n",
      "File dataset_1.txt has 25 windows, each of which has 1000 rows for activity: 10\n",
      "File dataset_1.txt has 24 windows, each of which has 1000 rows for activity: 11\n",
      "File dataset_1.txt has 24 windows, each of which has 1000 rows for activity: 12\n",
      "File dataset_1.txt has 12 windows, each of which has 1000 rows for activity: 13\n",
      "attempted to create training feature set data file:week6_training_data_1Participant.csv. Please check if the file was created successfully in your local folder!\n",
      "239 data rows should be in the output training feature set file\n",
      "File dataset_3.txt has 12 windows, each of which has 1000 rows for activity: 1\n",
      "File dataset_3.txt has 12 windows, each of which has 1000 rows for activity: 2\n",
      "File dataset_3.txt has 12 windows, each of which has 1000 rows for activity: 3\n",
      "File dataset_3.txt has 24 windows, each of which has 1000 rows for activity: 4\n",
      "File dataset_3.txt has 12 windows, each of which has 1000 rows for activity: 5\n",
      "File dataset_3.txt has 15 windows, each of which has 1000 rows for activity: 6\n",
      "File dataset_3.txt has 55 windows, each of which has 1000 rows for activity: 7\n",
      "File dataset_3.txt has 9 windows, each of which has 1000 rows for activity: 8\n",
      "File dataset_3.txt has 7 windows, each of which has 1000 rows for activity: 9\n",
      "File dataset_3.txt has 24 windows, each of which has 1000 rows for activity: 10\n",
      "File dataset_3.txt has 24 windows, each of which has 1000 rows for activity: 11\n",
      "File dataset_3.txt has 24 windows, each of which has 1000 rows for activity: 12\n",
      "File dataset_3.txt has 9 windows, each of which has 1000 rows for activity: 13\n",
      "attempted to create testing feature set data file:week6_testing_data_1Participant.csv. Please check if the file was created successfully in your local folder!\n",
      "239 data rows should be in the output testing set file\n",
      "Accuracy:  0.606694560669456\n",
      "[[ 0  0  0  2  0  0  0  0  0  0 10  0  0]\n",
      " [ 0 12  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  3  5  0  0  0  3  0  1  0  0  0  0]\n",
      " [ 0  0  0 20  2  1  0  0  0  0  0  1  0]\n",
      " [ 0  0  0  0 10  1  0  0  1  0  0  0  0]\n",
      " [ 0  0  0  6  2  5  0  0  2  0  0  0  0]\n",
      " [ 0  0  0  3  0  2 34  0 16  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  8  0  1  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  1  0  6  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0 24  0  0  0]\n",
      " [ 0  0  0  3  0  0  0  0  0  0 19  2  0]\n",
      " [ 0  1  0  1  0  1  0  0  0  0 20  1  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  9]]\n"
     ]
    }
   ],
   "source": [
    "#Code Block 2.1\n",
    "\n",
    "training_fileName = []\n",
    "training_fileName.append ('dataset_1.txt')\n",
    "testing_fileName = []\n",
    "testing_fileName.append ('dataset_3.txt')\n",
    "create_training_data_from_files (training_fileName, 'week6_training_data_1Participant.csv')\n",
    "create_testing_data_from_files (testing_fileName, 'week6_testing_data_1Participant.csv')\n",
    "\n",
    "df_training = pd.read_csv('week6_training_data_1Participant.csv', header=None)\n",
    "df_testing = pd.read_csv('week6_testing_data_1Participant.csv', header=None)\n",
    "\n",
    "label_train = df_training[9].values\n",
    "# Labels should start from 0 in sklearn\n",
    "label_train = label_train - 1\n",
    "df_training = df_training.drop([9], axis=1)\n",
    "data_train = df_training.values\n",
    "\n",
    "label_test = df_testing[9].values\n",
    "label_test = label_test - 1\n",
    "df_testing = df_testing.drop([9], axis=1)\n",
    "data_test = df_testing.values\n",
    "\n",
    "# Feature normalization for improving the performance of machine learning models. In this example code, \n",
    "# StandardScaler is used to scale original feature to be centered around zero. You could try other normalization methods.\n",
    "scaler = preprocessing.StandardScaler().fit(data_train)\n",
    "data_train = scaler.transform(data_train)\n",
    "data_test = scaler.transform(data_test)\n",
    "\n",
    "# Build KNN classifier, in this example code\n",
    "knn = KNeighborsClassifier(n_neighbors=4)\n",
    "knn.fit(data_train, label_train)\n",
    "\n",
    "# Evaluation. when we train a machine learning model on training set, we should evaluate its performance on testing set.\n",
    "# We could evaluate the model by different metrics. Firstly, we could calculate the classification accuracy.\n",
    "label_pred = knn.predict(data_test)\n",
    "print('Accuracy: ', accuracy_score(label_test, label_pred))\n",
    "# We could use confusion matrix to view the classification for each activity.\n",
    "print(confusion_matrix(label_test, label_pred))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### in the code block below, we changed the K neighbours in KNN to 6 instead of 4 in the pervious code section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Code Block 2.2\n",
    "df_training = pd.read_csv('week6_training_data_1Participant.csv', header=None)\n",
    "df_testing = pd.read_csv('week6_testing_data_1Participant.csv', header=None)\n",
    "\n",
    "label_train = df_training[9].values\n",
    "# Labels should start from 0 in sklearn\n",
    "label_train = label_train - 1\n",
    "df_training = df_training.drop([9], axis=1)\n",
    "data_train = df_training.values\n",
    "\n",
    "label_test = df_testing[9].values\n",
    "label_test = label_test - 1\n",
    "df_testing = df_testing.drop([9], axis=1)\n",
    "data_test = df_testing.values\n",
    "\n",
    "# Feature normalization for improving the performance of machine learning models. In this example code, \n",
    "# StandardScaler is used to scale original feature to be centered around zero. You could try other normalization methods.\n",
    "scaler = preprocessing.StandardScaler().fit(data_train)\n",
    "data_train = scaler.transform(data_train)\n",
    "data_test = scaler.transform(data_test)\n",
    "\n",
    "# Build KNN classifier, in this example code\n",
    "knn = KNeighborsClassifier(n_neighbors=6)\n",
    "knn.fit(data_train, label_train)\n",
    "\n",
    "# Evaluation. when we train a machine learning model on training set, we should evaluate its performance on testing set.\n",
    "# We could evaluate the model by different metrics. Firstly, we could calculate the classification accuracy.\n",
    "label_pred = knn.predict(data_test)\n",
    "print('Accuracy: ', accuracy_score(label_test, label_pred))\n",
    "# We could use confusion matrix to view the classification for each activity.\n",
    "print(confusion_matrix(label_test, label_pred))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In the following code, we will use two participants data as training and one participant as testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Code Block 2.3\n",
    "training_fileName = []\n",
    "training_fileName.append ('dataset_1.txt')\n",
    "training_fileName.append('dataset_3.txt')\n",
    "testing_fileName = []\n",
    "testing_fileName.append ('dataset_4.txt')\n",
    "create_training_data_from_files (training_fileName, 'week6_training_data_2Participant.csv')\n",
    "create_testing_data_from_files (testing_fileName, 'week6_testing_data_1Participant.csv')\n",
    "\n",
    "\n",
    "df_training = pd.read_csv('week6_training_data_2Participant.csv', header=None)\n",
    "df_testing = pd.read_csv('week6_testing_data_1Participant.csv', header=None)\n",
    "\n",
    "label_train = df_training[9].values\n",
    "# Labels should start from 0 in sklearn\n",
    "label_train = label_train - 1\n",
    "df_training = df_training.drop([9], axis=1)\n",
    "data_train = df_training.values\n",
    "\n",
    "label_test = df_testing[9].values\n",
    "label_test = label_test - 1\n",
    "df_testing = df_testing.drop([9], axis=1)\n",
    "data_test = df_testing.values\n",
    "\n",
    "# Feature normalization for improving the performance of machine learning models. In this example code, \n",
    "# StandardScaler is used to scale original feature to be centered around zero. You could try other normalization methods.\n",
    "scaler = preprocessing.StandardScaler().fit(data_train)\n",
    "data_train = scaler.transform(data_train)\n",
    "data_test = scaler.transform(data_test)\n",
    "\n",
    "# Build KNN classifier, in this example code\n",
    "knn = KNeighborsClassifier(n_neighbors=4)\n",
    "knn.fit(data_train, label_train)\n",
    "\n",
    "# Evaluation. when we train a machine learning model on training set, we should evaluate its performance on testing set.\n",
    "# We could evaluate the model by different metrics. Firstly, we could calculate the classification accuracy.\n",
    "label_pred = knn.predict(data_test)\n",
    "print('Accuracy: ', accuracy_score(label_test, label_pred))\n",
    "# We could use confusion matrix to view the classification for each activity.\n",
    "print(confusion_matrix(label_test, label_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In the following code, we will use four participants data as training and one participant as testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Code Block 2.4\n",
    "training_fileName = []\n",
    "training_fileName.append ('dataset_1.txt')\n",
    "training_fileName.append('dataset_3.txt')\n",
    "training_fileName.append('dataset_4.txt')\n",
    "training_fileName.append('dataset_5.txt')\n",
    "testing_fileName = []\n",
    "testing_fileName.append ('dataset_6.txt')\n",
    "create_training_data_from_files (training_fileName, 'week6_training_data_4Participant.csv')\n",
    "create_testing_data_from_files (testing_fileName, 'week6_testing_data_1Participant.csv')\n",
    "\n",
    "\n",
    "df_training = pd.read_csv('week6_training_data_4Participant.csv', header=None)\n",
    "df_testing = pd.read_csv('week6_testing_data_1Participant.csv', header=None)\n",
    "\n",
    "label_train = df_training[9].values\n",
    "# Labels should start from 0 in sklearn\n",
    "label_train = label_train - 1\n",
    "df_training = df_training.drop([9], axis=1)\n",
    "data_train = df_training.values\n",
    "\n",
    "label_test = df_testing[9].values\n",
    "label_test = label_test - 1\n",
    "df_testing = df_testing.drop([9], axis=1)\n",
    "data_test = df_testing.values\n",
    "\n",
    "# Feature normalization for improving the performance of machine learning models. In this example code, \n",
    "# StandardScaler is used to scale original feature to be centered around zero. You could try other normalization methods.\n",
    "scaler = preprocessing.StandardScaler().fit(data_train)\n",
    "data_train = scaler.transform(data_train)\n",
    "data_test = scaler.transform(data_test)\n",
    "\n",
    "# Build KNN classifier, in this example code\n",
    "knn = KNeighborsClassifier(n_neighbors=4)\n",
    "knn.fit(data_train, label_train)\n",
    "\n",
    "# Evaluation. when we train a machine learning model on training set, we should evaluate its performance on testing set.\n",
    "# We could evaluate the model by different metrics. Firstly, we could calculate the classification accuracy.\n",
    "label_pred = knn.predict(data_test)\n",
    "print('Accuracy: ', accuracy_score(label_test, label_pred))\n",
    "# We could use confusion matrix to view the classification for each activity.\n",
    "print(confusion_matrix(label_test, label_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Challenge 3\n",
    "\n",
    "#Question 1\n",
    "#\n",
    "#After running code block 2.1 where we change K value of the KNN model from 3 to 4 , compared to the results from running \n",
    "#code block 1.1, which actitivies have improved \n",
    "#accuracy and which activities has decreased accuracy?  How about the overall accuracy (improved or dropped)? \n",
    "\n",
    "\n",
    "#Question 2\n",
    "#\n",
    "#After running code block 2.2 where we further increase the value of model K value to 6 from 4, compared to the results from running \n",
    "#code block 2.1, does such change help in improving the accuracy? If so, which activities has improved accuracy? \n",
    "#How about the overal accuracy (improved or dropped)?\n",
    "\n",
    "\n",
    "#Question 3\n",
    "#\n",
    "#After running code block 2.3 where we used 2 participants data to train the main instead of using 1, compared to the results from running \n",
    "#code block 2.1, which actitivies have improved \n",
    "#accuracy and which activities has decreased accuracy?  How about the overall accuracy (improved or dropped)?\n",
    "\n",
    "\n",
    "\n",
    "#Question 4\n",
    "#\n",
    "#After running code block 2.4 where we used 4 participants data to train the model, compared to the results from running \n",
    "#code block 2.3, which actitivies have improved \n",
    "#accuracy and which activities has decreased accuracy?  How about the overall accuracy (improved or dropped)?\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code Section 3:  Testing with features and training data (E)\n",
    "## This section contains one challenge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In the following code, we will  use one participant data as training and one participant as testing, but we will introduce more features (page 42 from week 5 lecture notes) NOTE: in the lecture, there are two static features Min and Max, in the practical, there are three static featuers Min,Max, and Mean.  Don't memorize, understand the principle behind and apply"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File dataset_1.txt has 12 windows, each of which has 1000 rows for activity: 1\n",
      "File dataset_1.txt has 12 windows, each of which has 1000 rows for activity: 2\n",
      "File dataset_1.txt has 12 windows, each of which has 1000 rows for activity: 3\n",
      "File dataset_1.txt has 24 windows, each of which has 1000 rows for activity: 4\n",
      "File dataset_1.txt has 12 windows, each of which has 1000 rows for activity: 5\n",
      "File dataset_1.txt has 18 windows, each of which has 1000 rows for activity: 6\n",
      "File dataset_1.txt has 52 windows, each of which has 1000 rows for activity: 7\n",
      "File dataset_1.txt has 6 windows, each of which has 1000 rows for activity: 8\n",
      "File dataset_1.txt has 6 windows, each of which has 1000 rows for activity: 9\n",
      "File dataset_1.txt has 25 windows, each of which has 1000 rows for activity: 10\n",
      "File dataset_1.txt has 24 windows, each of which has 1000 rows for activity: 11\n",
      "File dataset_1.txt has 24 windows, each of which has 1000 rows for activity: 12\n",
      "File dataset_1.txt has 12 windows, each of which has 1000 rows for activity: 13\n",
      "attempted to create training feature set data file:week6_training_data_1ParticipantMorefeatures.csv. Please check if the file was created successfully in your local folder!\n",
      "239 data rows should be in the output training feature set file\n",
      "File dataset_3.txt has 12 windows, each of which has 1000 rows for activity: 1\n",
      "File dataset_3.txt has 12 windows, each of which has 1000 rows for activity: 2\n",
      "File dataset_3.txt has 12 windows, each of which has 1000 rows for activity: 3\n",
      "File dataset_3.txt has 24 windows, each of which has 1000 rows for activity: 4\n",
      "File dataset_3.txt has 12 windows, each of which has 1000 rows for activity: 5\n",
      "File dataset_3.txt has 15 windows, each of which has 1000 rows for activity: 6\n",
      "File dataset_3.txt has 55 windows, each of which has 1000 rows for activity: 7\n",
      "File dataset_3.txt has 9 windows, each of which has 1000 rows for activity: 8\n",
      "File dataset_3.txt has 7 windows, each of which has 1000 rows for activity: 9\n",
      "File dataset_3.txt has 24 windows, each of which has 1000 rows for activity: 10\n",
      "File dataset_3.txt has 24 windows, each of which has 1000 rows for activity: 11\n",
      "File dataset_3.txt has 24 windows, each of which has 1000 rows for activity: 12\n",
      "File dataset_3.txt has 9 windows, each of which has 1000 rows for activity: 13\n",
      "attempted to create testing feature set data file:week6_testing_data_1ParticipantMorefeatures.csv. Please check if the file was created successfully in your local folder!\n",
      "239 data rows should be in the output testing set file\n",
      "Accuracy:  0.7154811715481172\n",
      "[[ 1  5  5  0  0  0  0  0  0  0  1  0  0]\n",
      " [ 0 12  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  9  3  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  1 22  1  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  2 10  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  1  3 10  0  0  1  0  0  0  0]\n",
      " [ 0  0  0  0  0  0 55  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  6  3  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  7  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0 24  0  0  0]\n",
      " [ 0  0  0  5  0  0  0  0  0  0 17  2  0]\n",
      " [ 0  0  0  3  0  0  0  0  0  0 21  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  9]]\n"
     ]
    }
   ],
   "source": [
    "#Code Block 3.1\n",
    "\n",
    "def create_training_data_from_filesWithMoreFeatures(list_of_filenames, output_filename):\n",
    "\n",
    "    #create the empty training set where we are going to add our \"features\"\n",
    "    training_set = np.empty(shape=(0, 73))\n",
    "    \n",
    "    for dataset_file in list_of_filenames:\n",
    "\n",
    "        #import the file contents into a panadas data frame\n",
    "        imported_data = pd.read_csv(dataset_file, sep=',', header=None)\n",
    "\n",
    "        #generate \"features\" for each activitiy\n",
    "        for activityNumber in range(1,14):\n",
    "            \n",
    "            #get all data relating to that activity and convert to a numpy ndarray\n",
    "            activity_data = imported_data[imported_data[24] == activityNumber].values\n",
    "\n",
    "            #smooth over the data for columns 0, 1, 2, ...23 (not column 24)\n",
    "            b, a = signal.butter(4, 0.04, 'low', analog=False)\n",
    "            for j in range(24):\n",
    "                activity_data[:, j] = signal.lfilter(b, a, activity_data[:, j])\n",
    "            \n",
    "            #how many full rows of 1000 are there for this activity data?\n",
    "            number_of_training_windows = int( len(activity_data)/1000 )\n",
    "            print(  \"File \" + dataset_file +\n",
    "                    \" has \" + str(number_of_training_windows) + \" windows, each of which has 1000 rows \"+\n",
    "                    \"for activity: \" + str(activityNumber))\n",
    "            \n",
    "            #for each window of 1000 rows... scan the data and add the scan results to training_set\n",
    "            for window_number in range(number_of_training_windows):\n",
    "                #sample data (get the next 1000 rows and all the columns)\n",
    "                window_data = activity_data[ \n",
    "                                1000 * window_number : 1000 * (window_number + 1) , \n",
    "                                :\n",
    "                            ]\n",
    "                #we are about to build up a feature_sample that will have 73 columns, why?\n",
    "                feature_window = []\n",
    "                for i in range(24):\n",
    "                    feature_window.append(np.min(window_data[:, i]))\n",
    "                    feature_window.append(np.max(window_data[:, i]))\n",
    "                    feature_window.append(np.mean(window_data[:, i]))\n",
    "                # add the activtiy number (The last column from the row of data)\n",
    "                feature_window.append(int(window_data[0, -1])) \n",
    "                #make it in to an ndarray so it can be added to training data\n",
    "                feature_window = np.array([feature_window]) \n",
    "                training_set = np.concatenate((training_set, feature_window), axis=0)\n",
    "            \n",
    "    #now save all this training data into a file to be used at a later date\n",
    "    df_training = pd.DataFrame(training_set)\n",
    "    df_training.to_csv(output_filename, index=None, header=None)\n",
    "    print('attempted to create training feature set data file:'+ output_filename +'. Please check if the file was created successfully in your local folder!')\n",
    "    print(str(len(training_set)) + \" data rows should be in the output training feature set file\")\n",
    "\n",
    "\n",
    "def create_testing_data_from_filesWithMoreFeatures(list_of_filenames, output_filename):\n",
    "\n",
    "    #create the empty training set where we are going to add our \"features\"\n",
    "    testing_set = np.empty(shape=(0, 73))\n",
    "    \n",
    "    for dataset_file in list_of_filenames:\n",
    "\n",
    "        #import the file contents into a panadas data frame\n",
    "        imported_data = pd.read_csv(dataset_file, sep=',', header=None)\n",
    "\n",
    "        #generate \"features\" for each activitiy\n",
    "        for activityNumber in range(1,14):\n",
    "            \n",
    "            #get all data relating to that activity and convert to a numpy ndarray\n",
    "            activity_data = imported_data[imported_data[24] == activityNumber].values\n",
    "\n",
    "            #smooth over the data for columns 0, 1, 2, ...23 (not column 24)\n",
    "            b, a = signal.butter(4, 0.04, 'low', analog=False)\n",
    "            for j in range(24):\n",
    "                activity_data[:, j] = signal.lfilter(b, a, activity_data[:, j])\n",
    "            \n",
    "            #how many full rows of 1000 are there for this activity data?\n",
    "            number_of_testing_windows = int( len(activity_data)/1000 )\n",
    "            print(  \"File \" + dataset_file +\n",
    "                    \" has \" + str(number_of_testing_windows) + \" windows, each of which has 1000 rows \"+\n",
    "                    \"for activity: \" + str(activityNumber))\n",
    "            \n",
    "            #for each window of 1000 rows... scan the data and add the scan results to training_set\n",
    "            for window_number in range(number_of_testing_windows):\n",
    "                #sample data (get the next 1000 rows and all the columns)\n",
    "                window_data = activity_data[ \n",
    "                                1000 * window_number : 1000 * (window_number + 1) , \n",
    "                                :\n",
    "                            ]\n",
    "                #we are about to build up a feature_sample that will have 10 columns\n",
    "                feature_window = []\n",
    "                for i in range(24):\n",
    "                    feature_window.append(np.min(window_data[:, i]))\n",
    "                    feature_window.append(np.max(window_data[:, i]))\n",
    "                    feature_window.append(np.mean(window_data[:, i]))\n",
    "                # add the activtiy number (The last column from the row of data)\n",
    "                feature_window.append(int(window_data[0, -1])) \n",
    "                #make it in to an ndarray so it can be added to training data\n",
    "                feature_window = np.array([feature_window]) \n",
    "                testing_set = np.concatenate((testing_set, feature_window), axis=0)\n",
    "            \n",
    "    #now save all this training data into a file to be used at a later date\n",
    "    df_testing = pd.DataFrame(testing_set)\n",
    "    df_testing.to_csv(output_filename, index=None, header=None)\n",
    "    print('attempted to create testing feature set data file:'+ output_filename +'. Please check if the file was created successfully in your local folder!')\n",
    "    print(str(len(testing_set)) + \" data rows should be in the output testing set file\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#using four participants as training\n",
    "#and one participant as testing\n",
    "training_fileName = []\n",
    "training_fileName.append ('dataset_1.txt')\n",
    "testing_fileName = []\n",
    "testing_fileName.append ('dataset_3.txt')\n",
    "create_training_data_from_filesWithMoreFeatures (training_fileName, 'week6_training_data_1ParticipantMorefeatures.csv')\n",
    "create_testing_data_from_filesWithMoreFeatures (testing_fileName, 'week6_testing_data_1ParticipantMorefeatures.csv')\n",
    "\n",
    "\n",
    "df_training = pd.read_csv('week6_training_data_1ParticipantMorefeatures.csv', header=None)\n",
    "df_testing = pd.read_csv('week6_testing_data_1ParticipantMorefeatures.csv', header=None)\n",
    "\n",
    "label_train = df_training[72].values\n",
    "# Labels should start from 0 in sklearn\n",
    "label_train = label_train - 1\n",
    "df_training = df_training.drop([72], axis=1)\n",
    "data_train = df_training.values\n",
    "\n",
    "label_test = df_testing[72].values\n",
    "label_test = label_test - 1\n",
    "df_testing = df_testing.drop([72], axis=1)\n",
    "data_test = df_testing.values\n",
    "\n",
    "# Feature normalization for improving the performance of machine learning models. In this example code, \n",
    "# StandardScaler is used to scale original feature to be centered around zero. You could try other normalization methods.\n",
    "scaler = preprocessing.StandardScaler().fit(data_train)\n",
    "data_train = scaler.transform(data_train)\n",
    "data_test = scaler.transform(data_test)\n",
    "\n",
    "# Build KNN classifier, in this example code\n",
    "knn = KNeighborsClassifier(n_neighbors=4)\n",
    "knn.fit(data_train, label_train)\n",
    "\n",
    "# Evaluation. when we train a machine learning model on training set, we should evaluate its performance on testing set.\n",
    "# We could evaluate the model by different metrics. Firstly, we could calculate the classification accuracy.\n",
    "label_pred = knn.predict(data_test)\n",
    "print('Accuracy: ', accuracy_score(label_test, label_pred))\n",
    "# We could use confusion matrix to view the classification for each activity.\n",
    "print(confusion_matrix(label_test, label_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In the following code, we will  use four participants data as training and one participant as testing and the same number of features as above code block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Code Block 3.2\n",
    "\n",
    "#using four participants as training\n",
    "#and one participant as testing\n",
    "training_fileName = []\n",
    "training_fileName.append ('dataset_1.txt')\n",
    "training_fileName.append('dataset_3.txt')\n",
    "training_fileName.append('dataset_4.txt')\n",
    "training_fileName.append('dataset_5.txt')\n",
    "testing_fileName = []\n",
    "testing_fileName.append ('dataset_6.txt')\n",
    "create_training_data_from_filesWithMoreFeatures (training_fileName, 'week6_training_data_4Participant.csv')\n",
    "create_testing_data_from_filesWithMoreFeatures (testing_fileName, 'week6_testing_data_1Participant.csv')\n",
    "\n",
    "\n",
    "df_training = pd.read_csv('week6_training_data_4Participant.csv', header=None)\n",
    "df_testing = pd.read_csv('week6_testing_data_1Participant.csv', header=None)\n",
    "\n",
    "label_train = df_training[72].values\n",
    "# Labels should start from 0 in sklearn\n",
    "label_train = label_train - 1\n",
    "df_training = df_training.drop([72], axis=1)\n",
    "data_train = df_training.values\n",
    "\n",
    "label_test = df_testing[72].values\n",
    "label_test = label_test - 1\n",
    "df_testing = df_testing.drop([72], axis=1)\n",
    "data_test = df_testing.values\n",
    "\n",
    "# Feature normalization for improving the performance of machine learning models. In this example code, \n",
    "# StandardScaler is used to scale original feature to be centered around zero. You could try other normalization methods.\n",
    "scaler = preprocessing.StandardScaler().fit(data_train)\n",
    "data_train = scaler.transform(data_train)\n",
    "data_test = scaler.transform(data_test)\n",
    "\n",
    "# Build KNN classifier, in this example code\n",
    "knn = KNeighborsClassifier(n_neighbors=4)\n",
    "knn.fit(data_train, label_train)\n",
    "\n",
    "# Evaluation. when we train a machine learning model on training set, we should evaluate its performance on testing set.\n",
    "# We could evaluate the model by different metrics. Firstly, we could calculate the classification accuracy.\n",
    "label_pred = knn.predict(data_test)\n",
    "print('Accuracy: ', accuracy_score(label_test, label_pred))\n",
    "# We could use confusion matrix to view the classification for each activity.\n",
    "print(confusion_matrix(label_test, label_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Challenge 4\n",
    "\n",
    "\n",
    "#Question 1\n",
    "#\n",
    "#After running code block 3.1 where we increase number of features, compared to the results from running \n",
    "#code block 1.1, which actitivies have improved \n",
    "#accuracy and which activities has decreased accuracy?  How about the overall accuracy (improved or dropped)? How many \n",
    "#features have been added? what are these newly added features?\n",
    "\n",
    "\n",
    "#Question 2\n",
    "#\n",
    "#After running code block 3.2 where we increase number of training data, compared to the results from running \n",
    "#code block 2.4, which actitivies have improved \n",
    "#accuracy and which activities has decreased accuracy?  How about the overall accuracy (improved or dropped)?\n",
    "\n",
    "\n",
    "#Question 3\n",
    "#\n",
    "#It seems number of features and number of training data are important, but in real-world, how do you know which\n",
    "#features are useful and how do you collect more training data?  How many are quantified as sufficient training data?\n",
    "\n",
    "\n",
    "#Question 4\n",
    "#\n",
    "#How to do boundary value analysis for this activity recognition system? What about code coverage?\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
